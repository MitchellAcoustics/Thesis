\chapter[Sound Source Information in Predictive Models]{Sound Source Information in Predictive Models\footnote{The content of this chapter was originally published as \citet{Orga2021Multilevel}, a collaborative work between myself from the SSID team at UCL and Dr Ferran Orga, a researcher at Grup de Recerca en Tecnologies M{\'e}dia, La Salle-URL. Dr Orga and I shared first authorship on this paper. Original data collection was performed by the team at La Salle-URL while the data analysis and modelling strategy was conceived by the team at UCL and implemented by me. Dr Orga and myself drafted the original manuscript, with my work focussing on the analysis method section, results, and discussion of the modelling results.}}
\label{ch:mlmann}

% \section*{Abstract}

% \copied{The recent development and deployment of Wireless Acoustic Sensor Networks (WASN) present new ways to address urban acoustic challenges in a smart city context. A focus on improving quality of life forms the core of smart-city design paradigms and cannot be limited to simply measuring objective environmental factors, but should also consider the perceptual, psychological, and health impacts on citizens. This study therefore makes use of short (1 - 2.7s) recordings sourced from a WASN in Milan which were grouped into various environmental sound source types and given an annoyance rating via an online survey with $N=100$ participants. A multilevel psychoacoustic model was found to achieve an overall $R^2=0.64$ which incorporates Sharpness as a fixed effect regardless of the sound source type and Roughness, Impulsiveness, and Tonality as random effects whose coefficients vary depending on the sound source. These results present a promising step torward implementing an on-sensor annoyance model which incorporates psychoacoustic features and sound source type, and is ultimately not dependent on sound level.}

In \citet{Brown2009acoustic}, the author proposes that one of the underutilised concepts in making use of sound as a resource is the disaggregation of sound sources. He states that `the type of sound sources present is critical in judgements about outdoor sound quality'. The goal is to move away from the straightforward use of aggregate sound metrics, which attempt to summarise the sound environment as a whole, through various acoustic metrics. Given the various semantic meanings that listeners associate with certain sound sources, the annoyance elicited by particular sounds will vary as will the relationship between the acoustic features of that sound and the annoyance \citep{LafayInvestigating}.

Given the nature of the \gls{isd} as a large dataset containing hundreds of \emph{in-situ} recordings, identifying sound sources manually was impractical at this stage. In order to progress towards a sound-source aware model, I therefore partnered with researchers from the LIFE DYNAMAP project who had curated a set of labelled recordings selected from a \gls{wasn} installed in Milan (Italy). For this study, the DYNAMAP researchers asked more than 100 people to conduct three different perceptual tests through an online survey \citep{AlsinaPages2021Perceptual}. 

The perceptual tests were designed to measure the annoyance in people relating to different urban sounds and their characteristics \citep{LabairuTrenchs2018Noise,AlsinaPages2021Perceptual}, by means of short excerpts of raw acoustic audio obtained from the DYNAMAP project \citep{Sevillano2016DYNAMAP}. The audio excerpts which were most representative of the site were selected, using a wide range of sound types (sirens, airplanes, people talking, dogs barking, etc.) \citep{Alias2020Aggregate,Alias2020WASN}. Sound annoyance depends on the acoustic characterisation of each sample, and it is possible to classify the acoustic excerpts depending on their sound source characterisation, which can be the basis to ask participants about their perceptions. The psychoacoustic characterisation is based on the psychoacoustic measurements of loudness, sharpness, and others defined by \citet{PsychoacousticsfactsmodelsZwicker}.

Based on the data collected by the DYNAMAP team, I aim to determine the psychoacoustic parameters that have an effect in the individual annoyance scores, and how the relationships between these parameters and annoyance may vary according to the dominant sound source. For this reason, a multilevel psychoacoustic model is trained using the results of the \glsfirst{mushra} test \citep{IRB2015Method}, focused on annoyance evaluation by the participants over several different types of sound. The results show that sound source identification provides valuable information for a predictive model and that sharpness is a primary predictor for annoyance which is independent of the sound source.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}

% In this section, I detail the several methods applied in this experiment from the perceptual test design based on an urban sound dataset \cit{21} to the multilevel linear regression modelling applied to obtain the annoyance prediction.

\subsection{Dataset}
\label{sec:DYANAMAPDataset}
This study makes use of a dataset collected in collaboration with the LIFE DYNAMAP\footnote{The data collection (both the collection of the recordings and the online survey) was performed by the DYNAMAP team. To maintain consistency with the published version of this study and to provide the appropriate context, the text on data collection (\cref{sec:DYANAMAPDataset,sec:DYNAMAPTests}) has been reproduced verbatim from our study \citep{Orga2021Multilevel} and was initially drafted by Dr. Orga, the other first author.} project conducted in Milan (Italy) \citep{Sevillano2016DYNAMAP,Alias2020WASN}. This project makes use of a \gls{wasn}, enabling the collection of data over a longer period of time than was possible with the \gls{ssid} protocol outlined in \cref{chap:protocol}. A \gls{wasn} enables a broader characterisation of the acoustic events present in a location, as recording conditions can be made consistent across the nodes and data can be retrieved at any time of the day.

The dataset used in this study has been obtained by homogeneously sampling several hours, in both weekday and weekend, with 24 sensors distributed along the urban District 9 of Milan \citep{AlsinaPages2018Detection}. After that, experts from the DYNAMAP development team labelled the acoustic events of the recordings manually to obtain a 151-h dataset \citep{Alias2020WASN}. Due to the nature of the project, this consisted in removing events not related to traffic noise from the noise map computation, events were grouped in \gls{rtn} that belongs to the 83.7\% of the total time of the dataset, and \gls{ane} with the 8.7\% of the total time. Another class was used to include overlapping and unidentified events: \gls{complx} with 7.6\% of the total time \citep{Alias2020Aggregate}. During the labelling process, the DYNAMAP developers found up to 26 types of anomalous events, which they decided to group into the following classes: airplane, alarm, bell, bike, bird, blind, brake, bus door, construction, dog, door, glass, horn, interference, music, people, rain, rubbish service, siren, squeak, step, thunder, tramway, train, trolley, wind, works (construction) \citep{Alias2017Description}.

The most common sound classes were picked to evaluate the relationship between the event measurements and the citizens' perception of annoyance. These selected events used in the study belong to the following 9 classes: airplane, bird, brake, construction, dog, door, horn, people, and siren \citep{Orga2017Impact}. As the selected events are the most common, those are the ones that contain the widest variety of recording conditions, including different sensor locations and recording hours \citep{LabairuTrenchs2018Noise}. The reason for that choice was two-fold:

\begin{enumerate}
  \item the availability of a wide range of examples of each type of sound to choose for the design of the tests, including the possibility of finding different samples that keep similar psychoacoustic values,
  \item the fact that the most common sounds are the most reasonable to evaluate with people, as they best summarise the character of the soundscape around each sensor.
\end{enumerate}
% Particularly rewrite this paragraph, it's a very different style
% \copied{The comparison between events was only carried out with sounds collected using the same sensor, in order to ensure the same recording conditions. For this reason, if the chosen events for the perceptive tests belong to a sensor or another, depends on the availability of the classes to be compared in each sensor. In all the cases, measures were taken to ensure that the sensor containing the events has enough variety of samples with various psychoacoustic parameters, to ensure a proper representation of each category. To satisfy these requirements, only data from four sensors have been used to make the comparisons, as they provide enough information to carry out the perceptual test, i.e. hb115, hb124, hb127, and hb133 \cit{20}. }
More details about the event selection process and the availability of the study sensors are detailed in \citep{LabairuTrenchs2018Noise}, and the time of each event in the sensors is depicted in \citep{AlsinaPages2021Perceptual}.

\subsection{Design of the perceptual tests}
\label{sec:DYNAMAPTests}
In order to assess the degree of annoyance produced by the aforementioned classes of sounds, an online test was conducted using the Web Audio Evaluation Tool \citep{Jillings2015Web}. Specifically, the \gls{mushra} test method \citep{IRB2015Method} -- which was originally designed for the evaluation of audio codecs -- has been adapted for this purpose. Participants were given a clear explanation of what they were going to be asked, including detailed instructions on the operation of the test. No training phase was therefore included. A demographic survey was included at the beginning of the test for all 100 participants, asking for them to identify their age, gender, and a subjective rating of the participant's residential area (zr1 - very quiet, zr2 - quiet, zr3 - bit noisy, zr4 - noisy, zr5 - very noisy).

The second part of the test consists of five sets. Each set presents a group of short acoustic events with similar values of loudness and sharpness but from different classes, and recorded in the same sensor, in order to maintain the recording conditions and location of the sounds under comparison. For each set, the participants were asked to evaluate the annoyance produced by the presented recordings, ordering them in a $0-10$ scale, where zero corresponds to \emph{not at all} and 10 corresponds to \emph{extremely disturbing} following the ICBEN recommendation. The interface was customised including a colour scale to help the participants place the stimuli according to the degree of annoyance that they perceive. Each audio is represented with a green bar with a `play' icon on it and the recordings are sorted randomly along the \gls{mushra} scale (see \cref{fig:mushra-test}). An audio recording is reproduced when the corresponding bar is clicked. The system ensures the participant listens to all the recordings and moves all the bars before they jump to the next set of recordings. The sets were presented in a random order to prevent learning biases and ordering effects. \gls{mushra} tests usually include hidden reference stimuli, which in audio or speech quality evaluation corresponds to the highest quality samples and that are used to remove outlier responses. Since stimuli pertaining to different classes are compared, no audio reference was included, thus avoiding biases towards a certain audio class. The participants were asked to take the test using headphones and to keep the same volume during all the tests, to maintain the same conditions throughout the entire testing process. One hundred participants undertook this test, 59 men and 41 women, with a mean age of 33. Participants were volunteers, mainly from the university and also gathered via social networks. The distribution according to residential area is the following: 9 in zr1, 37 in zr2, 35 in zr3, 18 in zr4, and 1 in zr5. The \gls{mushra} test allows us to:

\begin{enumerate}
  \item obtain an individual score of annoyance for each audio,
  \item carry out comparisons among the different types of events contained in a set.
\end{enumerate}

The detail of the stimuli included in each of the five sets of the test can be found in \cref{tab:sensor-stimuli}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{Figures/mushraScreenShot02.png}
  \caption{Screenshot of the \gls{mushra} test conducted to assess the annoyance provoked by different sounds. Title: sort the following sounds according to the cause annoyance. The scale ranges from \emph{not annoying at all} to \emph{extremely annoying}.  \label{fig:mushra-test}}
\end{figure}

\subsection{Psychoacoustic data analysis}

The dataset resulted in 27 audio recordings of identified sound events with durations ranging between 1.01 and 2.69 s. The calibrated audio files were imported into the ArtemiS Suite software (v. 11.5, Head Acoustics GmbH) and the following psychoacoustic parameters were computed: \emph{loudness, sharpness, roughness, tonality} and \emph{impulsiveness} \citep{PsychoacousticsfactsmodelsZwicker}; values for these parameters are reported in \cref{tab:sensor-stimuli}. The rationale for selecting a relatively large set of psychoacoustic metrics is that they are often used as indicators to predict perceptual constructs (such as annoyance) in perceptual studies, as shown in recent soundscape literature \citep{Aletta2016Soundscape,Aletta2017Dimensions}. Fluctuation Strength, which could otherwise be included in this list of psychoacoustic parameters, as in Zwicker's annoyance model, was not included as the length of the recordings are too short to obtain a valid value. Loudness was calculated according to the DIN 45631 / A1 standard for time-varying sounds, in a free-field \citep{DIN2008Calculation}. As recommended by the standard, in order to avoid the under-estimation of evaluated loudness which is seen when using the arithmetic average of the loudness curve, the \glsfirst{n5} value (the 5\% percentile value of the time-dependent loudness curve) is used as the single value of loudness. \glsfirst{s} was calculated according to \citet{DIN2009measurement}, in a free-field. With this sharpness method, the absolute loudness of the sound is not accounted for, so there should not be a duplication of information across the loudness and sharpness metrics. \glsfirst{r} was calculated according to the hearing model by \citet{Sottek2017Sound}, with the option to skip the first 0.5 s in order to not distort the single value. \glsfirst{iu} was also calculated according to the hearing model by Sottek, with a 0.5 s skip interval. Finally, \glsfirst{tu} was calculated according to \citet{ECMA742019Measurement}, which is based on the hearing model of Sottek, with a frequency range of 20 Hz to 20 kHz.


\begin{table}
\centering
\caption{Psychoacoustic parameters calculated for the 27 stimuli used in the listening experiment. \label{tab:sensor-stimuli}}
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccc} 
\toprule
\multirow{2}{*}{\textbf{Sensor }} & \multirow{2}{*}{\textbf{Label }} & \multicolumn{5}{c}{\textbf{Psychoacoustic Parameters }} \\ 
\cline{3-7}
 &  & \begin{tabular}[c]{@{}c@{}}\textbf{Loudness}\\\textbf{($N_5$ sone)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Sharpness}\\\textbf{(acum)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Roughness}\\\textbf{(asper)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Tonality}\\\textbf{(tuHMS)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Impulsiveness}\\\textbf{(iu)}\end{tabular} \\ 
\hline
hb133 & peop & 15.1 & 1.46 & 0.032 & 0.204 & 0.270 \\
hb133 & door & 16.8 & 1.43 & 0.029 & 0.113 & 0.354 \\
hb133 & dog & 13.1 & 1.22 & 0.033 & 0.373 & 0.266 \\
hb133 & brak & 16.0 & 1.76 & 0.030 & 0.326 & 0.241 \\
hb133 & bird & 12.6 & 1.73 & 0.024 & 0.283 & 0.214 \\
hb133 & airp & 13.0 & 1.27 & 0.060 & 0.438 & 0.231 \\ 
\hline
hb127 & sire & 17.7 & 1.56 & 0.045 & 1.540 & 0.178 \\
hb127 & peop & 16.1 & 1.62 & 0.035 & 0.410 & 0.417 \\
hb127 & horn & 18.1 & 1.56 & 0.028 & 0.666 & 0.260 \\
hb127 & door & 19.8 & 1.72 & 0.037 & 0.037 & 0.479 \\
hb127 & brak & 19.0 & 1.95 & 0.034 & 0.251 & 0.281 \\ 
\hline
hb127 & sire & 20.1 & 1.73 & 0.046 & 1.670 & 0.288 \\
hb127 & peop & 22.0 & 1.96 & 0.036 & 0.322 & 0.452 \\
hb127 & horn & 19.9 & 2.16 & 0.034 & 1.290 & 0.336 \\
hb127 & brak & 21.0 & 1.81 & 0.030 & 1.170 & 0.285 \\
hb127 & airp & 24.4 & 1.65 & 0.056 & 0.172 & 0.446 \\ 
\hline
hb115 & wrks & 20.3 & 1.97 & 0.054 & 0.227 & 0.267 \\
hb115 & trck & 24.4 & 1.60 & 0.033 & 0.040 & 0.276 \\
hb115 & sire & 19.5 & 1.46 & 0.054 & 0.861 & 0.333 \\
hb115 & peop & 25.1 & 1.79 & 0.032 & 0.411 & 0.331 \\
hb115 & horn & 22.3 & 2.00 & 0.032 & 0.806 & 0.155 \\
hb115 & door & 26.3 & 1.62 & 0.038 & 0.045 & 0.397 \\
hb115 & brak & 20.6 & 1.93 & 0.034 & 0.216 & 0.313 \\ 
\hline
hb115 & wrks & 24.6 & 1.92 & 0.064 & 0.447 & 0.317 \\
hb115 & sire & 26.6 & 1.77 & 0.044 & 0.626 & 0.290 \\
hb115 & horn & 29.5 & 2.35 & 0.039 & 0.486 & 0.262 \\
hb115 & door & 31.3 & 1.88 & 0.048 & 0.223 & 0.402 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Multi-level linear regression modelling}

The analysis for this study utilises a \gls{mlm}, with a varying intercept and a varying slope, using backward step feature selection. For this study, the data are grouped into two non-nested sets to form a two-level model: by repeated measures per respondent (\emph{user}) and by sound type (\emph{label}). In order to take into account repeated measures across participants, and to correct for the participant's mean annoyance level, the \emph{user} variable is included in the second-level as a varying intercept. We then include the psychoacoustic features as label effects, with coefficients which are allowed to vary across the sound type labels. The psychoacoustic features are also included as fixed effects in the first level, which do not vary across either the user or label groups.

The initial model structure, as written in Wilkinson-Rogers notation \citep{Wilkinson1973Symbolic}, is thus:

\begin{equation}
  % \begin{split}
  \label{eqn:initMLMann}
      Annoyance \sim N_5 + R + S + T + I + (1 | user) + (1 + N_5 + R + S + T + I | label)
  % \end{split}
  \end{equation}

\subsubsection{Feature selection}

The \gls{mlm} is initially fitted with all of the potential features included within both levels. In order to reduce the complexity of the model, a backwards step features selection process is applied to both levels of the model, in the manner described in \cref{sec:featureSelection}. This process involves fitting the full model which includes all of the potential independent features (i.e. \cref{eqn:initMLMann}). The feature with the highest \emph{p}-value (least significant)\footnotemark{} is then removed from the candidates and the model is refit. This process is repeated until all features meet the predefined significance threshold of $p < 0.05 $. For a two-level model, first backward elimination of the second level is performed, followed by backward elimination of the first-level (or fixed) part. 

%%%%%%%%%%%%
\footnotetext{Note that this process differs slightly from that taken in \cref{ch:lockdown} which used the \gls{aic} as an overall performance metric as opposed to a \emph{p}-value per feature method used here. In this case, this change represents a development of my modelling expertise, as the study presented in this chapter was actually completed before (2021) the study presented in \cref{ch:lockdown} (2022). While both are valid methods and may have different ideal applications, for a prediction-focussed approach, the \gls{aic} selection method is more approrpriate.}
%%%%%%%%%%%%


Multicollinearity is checked using \gls{vif} as described in \cref{chap:methods}. Once the feature selection process is completed, the final model with only significant features of interest included is fit and the table of the model coefficients is printed along with plots of the random effects and standardised estimates terms. Finally, quantile plots of the residuals and random effects are examined to confirm they are normally distributed \citep{Harrison2018brief}.

The input and output features are z-scaled prior to the analysis and model building by subtracting the mean and dividing by the standard deviation in order to directly compare the coefficient values of independent variables measured on different scales \citep{Harrison2018brief}. The model fitting and feature selection was performed using the \texttt{step} function from \texttt{lmerTest} (v. 3.1.3) \citep{Kuznetsova2017lmerTest} in the R statistical software (v. 4.0.5) \citep{RCT2018R}. The summaries and plots were created using the \texttt{sjPlot} package (v. 2.8.7) \citep{Luedecke2021sjPlot} and the multi-level $R^2$ values were calculated using \texttt{MuMIn} (v. 1.43.17) \citep{Barton2020MuMIN}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

\subsection{Differences in annoyance between groups \label{sec:groupDiffs}}

\footnote{The analysis carried out in \cref{sec:groupDiffs} was performed by Dr. Francesco Aletta. These results are included here verbatim from the original published paper to provide context for the later discussion on the influence of demographic differences on soundscape perception.}The average annoyance score of all users across all stimuli was $M=0.58 (SD=0.05)$. Since some basic demographic information about the 100 participants of the perceptual test was known, it seemed logical to explore possible differences in annoyance scores between different groups/levels of stratification of the sample, mostly for descriptive purposes. Therefore, Areas of residence and Gender were considered as factors in this analysis. Gender was treated as a binary variable (F/M), while Areas of residence was treated as a five-level categorical variable based on people's self-reported character of the area where they typically reside (range: 1-5; very quiet-very noisy). One-way repeated measures \gls{anova} was deemed to be the most appropriate approach to take into account the multiple responses that each of the 100 participants provided for the different recordings ($N=27$). A first analysis was then conducted to determine whether there was a statistically significant difference in annoyance between Areas of residence: no statistically significant differences were observed in this case with $F(4.95)=1.374, p=0.249$. Likewise, a second one-way repeated measures \gls{anova} was carried out to check whether statistically significant differences in annoyance existed between females and males: no statistically significant effect was observed in this case either $F(1.98)=0.714, p=0.400$. Such small differences between groups can indeed be observed in \cref{fig:anova}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{Figures/orga_anovas.png}
  \caption{Estimated Marginal Means for Annoyance as a function of Areas of residence (\textbf{left}) and Gender (\textbf{right}). \label{fig:anova}
}
\end{figure}

\subsection{Annoyance model}
In the context of the multi-level linear regression modelling, the included variables were assumed to have an effect at two levels: the first level (i.e. fixed effect(s)), and the second level, where annoyance score intercepts are allowed to vary as a function of users (i.e. the 100 participants), and where each feature of interest is allowed its own coefficient as a function of labels (i.e. the 7 types of sounds). Sharpness came up as the main predictor with a strong statistical significance in the fixed-effect level, as reported in Table \ref{tab:annoyance-model}. This implies that, regardless of any other factors, the sharper the sounds, the more annoying they are perceived to be.

\begin{table}[h]
  \centering
  \caption{Varying-intercept varying-slope multi-level model of psychaocoustic annoyance, accounting for repeated measures (user) and sound source type (label) within the second level. Coefficients and confidence intervals given are for z-scaled data. \label{tab:annoyance-model}}
  \begin{tabular}{cccc} 
  \toprule
   & \multicolumn{3}{c}{\textbf{Annoyance }} \\ 
  \hline
  \textit{Predictors} & \textit{Estimates} & \textit{CI} & \textit{p} \\ 
  \hline
  (Intercept) & 0.02 & -0.13 -- 0.16 & 0.811 \\
  Sharpness & 0.33 & 0.25 -- 0.40 & \textbf{\textless{}0.001} \\ 
  \hline
  \textbf{Random Effects} &  &  &  \\ 
  \hline
  $\sigma^2$ & 0.47 &  &  \\
  $\tau_{00user}$ & 0.28 &  &  \\
  $\tau_{00label}$ & 0.02 &  &  \\
  ICC & 0.39 &  &  \\
  $N_{user}$ & 100 &  &  \\
  $N_{label}$ & 10 &  &  \\ 
  \hline
  Observations & 2700 &  &  \\
  Marginal $R^2$ / Conditional $R^2$ & 0.08 / 0.64 &  &  \\
  \bottomrule
  \end{tabular}
  \end{table}

The second-level effects presented in \cref{fig:annoyance-effects} show that level- and loudness-based acoustic parameters do not play a significant role in predicting annoyance when considering other psychoacoustic factors and specific sound sources. However it should be noted that this may be influenced by the online data collection paradigm used in this study, which may struggle to control for the playback level. The variables selected by the feature selection algorithm within the type of sound (\emph{label}) level include: impulsiveness, roughness, tonality, and type of sound are relatively small, while roughness appears to be more important. For instance, when other effects are controlled for, the sound type `horn' seems to be less annoying the rougher it is; while for the types of sound `bird' and `siren', higher roughness values will lead to higher annoyance scores. Looking at the model from the point of view of the types of sound, one could observe that `horns' tend to be more annoying than other sounds if they are more impulsive, while `people' or `birds' or `brakes' result in more annoying scores compared to other sounds if their tonal components are more prominent. Overall, for this model, the marginal and conditional $R^2$ values are 0.08 and 0.64, respectively. Marginal $R^2$ provides the variance explained by the fixed effects only, and conditional $R^2$ provides the variance explained by the whole model, i.e. both fixed effects and second-level effects. Thus, the majority of variance is explained by second-level factors, while a smaller portion (8\%) is covered by sharpness alone.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{Figures/OrganMLMAnnoyanceRandom.png}
  \caption{Second-level effects figures representing the regression coefficients by types of sound (label) and for different psychoacoustic parameters. \label{fig:annoyance-effects}
}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

The fact that no significant differences in annoyance scores were observed between sample groups (i.e. gender or area of residence) is particularly interesting: it is common to assume in soundscape studies that personal and contextual factors play a strong role in how people respond to urban acoustic environments \citep{Kang2016Ten}. However, this is probably more relevant when complex sound environments (e.g. multi-source) are being considered and when dealing with relatively longer duration of exposures (e.g. several minutes) as seen in in-situ surveys. For clearly identifiable sources of environmental noise, with signals of short duration (i.e. 1-3s) like those used for this experiment, it is likely it was easier for the sample to converge on similar annoyance scores, regardless of other demographic factors. This aspect will be further investigated in \cref{ch:whostudy}.

\subsection{Psychoacoustic features}

Regarding the noise annoyance scores, sharpness came up as an important predictor in the first level of the modelling stage (explaining up to 8\% of the variance alone). It is important to highlight that the sharpness calculation method used in this study did not include any loudness correction; nor was any loudness-related parameter selected by the feature selection algorithm. To some extent, this is possibly due to the fact that, being an online experiment, it was not possible for the research team to actually calibrate the loudness playback level accurately for the remote participants. On the other hand, considering this aspect from the \gls{wasn} implementation perspective, this could be seen as an encouraging finding. As calibrating a diffuse acoustic monitoring network may not be practical in real-world scenarios, it is good to have models that can achieve up to 64\% of variance explained regardless of actual levels. Furthermore, in complex acoustic environments, loudness would likely vary over time depending on the relative positions between sound sources and (human) listeners in ways in which the other psychoacoustic parameters such as sharpness and tonality are less likely to. This is something that is impossible for fixed sensors to take into account, so once again it is preferable not to rely on loudness as a predictor.

This result also appears to differ from some of the results in the model from \cref{ch:lockdown}, where sharpness was not selected as a final feature in the \gls{isopl} model. There are a few potential explanations for this. First, although within the circumplex pleasantness is considered the opposite of annoying, the respondents' annoyance rating in this study may be focussed on more specific or different factors than what is captured in the combined \gls{isopl} score. Second, since the \gls{isd} data is collected \emph{in-situ}, the general pleasantness of the soundscape may emphasise different acoustic features than the online study procedure used in this study. Finally, the structure of this model effectively controls for the influence of sound source information, whereas the \cref{ch:lockdown} model has no source information included. This could be interpreted as the sound-source-aware model more correctly identifying the acoustic feature that is important, independent of the sound source. Whereas the previous model's feature selection results may be more likely to select the features which help to differentiate sound sources, which is not necessary in the sound-source-aware model. This would indicate that the features selected in the \cref{ch:lockdown} model were selected because they perform better at differentiating sound sources and accounting for the semantic meaning associated with sound sources, but when the semantic information itself is included in the model, other acoustic features are more important for determining annoyance. 

Being able to predict noise annoyance from recorded sounds is particularly helpful from a public health perspective. In the context of a smart-city framework, one could imagine a \gls{wasn} large enough to cover a whole urban area; having a noise annoyance prediction algorithm at the node position that can return live annoyance scores to a central server from sounds recorded locally by the sensor would make for a useful application for environmental protection officers and other stakeholders at community or local authority level \citep{Kang2018Impact}. A relevant issue to consider from the \gls{wasn} perspective, is that in previous studies conducted in both urban \citep{Alias2020WASN} and suburban \citep{Alias2020Aggregate} environments, a clear influence of the type of environment around the sensor location on the types of noise was seen. Not all urban and suburban locations around the sensors have frequent sirens or horns. The presence of these sounds depends on the most common activities (e.g. leisure, hospitals, etc.), the type of road (wide, narrow), and the type of buildings and houses surrounding the location. The types of sounds and their relative frequency of occurence can vary widely given the combination of these architectural and landscape characteristics. In the design of a general model for quality of life, the approach for considering sound source information presented in this chapter should be combined with the proposal for incorporating this architectural and contextual information developed previously in \cref{ch:bayes}.

\section{Incorporating into the general model}
As demonstrated above, incorporating information about the sound source can greatly improve the prediction of perceived annoyance. The modelling structure used in this chapter effectively created separate, sound-source-dependent models of psychoacoustic annoyance, in contrast to the general annoyance model developed by \citet{PsychoacousticsfactsmodelsZwicker}. Each sound source (traffic, horns, people, etc.) has its own linear combination of psychoacoustic values to demonstrate how the semantic meaning which the listener assigns to a sound mitigates the perceptual mapping from physical inputs to perceptual outcomes. Incorporating this information, such that this semantic meaning influences the outcome of the ideal predictive model, is crucial. 

To demonstrate how this information could be integrated into the overall model, we can return to the model created for \cref{ch:lockdown}. For the purposes of this, we will assume that sound source labels have been derived for the \gls{isd} data in the same way as was done for the DYNAMAP dataset, resulting in one label per recording. This creates quite a complex series of relationships, where not only do we expect the relationship between a psychoacoustic feature and the perception to change depending on the sound source, but also according to the location-context, and in such a way that the location-context may change either the direct psychoacoustic$\rightarrow$perception mapping or the source+psychoacoustic$\rightarrow$perception mapping. To integrate this series of relationships into the \cref{ch:lockdown} model we can include the sound source label as an interaction term at both the first and second (location) level\footnote{Here, I use \gls{n5} as a stand-in for the various psychoacoustic features we could consider. This could be substituted with any of the other features discussed or with a set of features.}:

\begin{equation}
  \label{eqn:integSource}
      ISOPl \sim 1 + N_5 + N_5 \cdot label + (N_5 + N_5 \cdot label | LocationID)
  \end{equation}

where \gls{n5} is used as a stand-in for the various psychoacoustic features we could consider; \emph{label} is the sound source label.

Since the sound source information is not (yet) available for the \gls{isd}, it is not feasible at this point to run an example of this model on the data to demonstrate its performance. This further development forms part of my future work.  What is still to be determined is 1) how this process can be automated, with minimal manual input from the model's users; and 2) how to deal with complex scenes where multiple overlapping sound sources are present and how to integrate this into the model.

The first point is simpler, conceptually, but possibly more difficult in practice. An automated sound source recognition algorithm could be used, such as YAMNET \citep{Hershey2017CNN}, based on the AudioSet ontology \citep{Gemmeke2017Audio} (available as a pre-trained model on MATLAB or for Python in Tensorflow Hub), or a model more specialised for urban environments, such as those presented in the \gls{dcase} challenges \citep{Bello2019SONYC} using the UrbanSound 8k dataset \citep{Salamon2014Dataset}. In general, these models slice the recording into short chunks (typically on the order of 1 second) and produce a relative weighting for the likely presence of each available sound class within that second. From this list of weightings, the top predicted sound sound can be extracted for that chunk. If necessary, these can then be summed to give the overall top predicted sound over the entire recording. A model such as this could be integrated into the data processing pipeline to first identify the sound source(s) for a given time step, before combining this with the psychoacoustic analyses for the same recording and fed into the above model. There is also a parallel field of research specifically into polyphonic sound event detection, which would likely suit this task more closely \citep{Mesaros2016Metrics}. This would enable a (theoretically) fully automated, sound-source aware, psychoacoustic model of soundscape perception. However, the lingering question -- and the particular reason that this approach wasn't taken from the start of this research given the state-of-the-art at the time -- is whether these environmental sound recognition models are accurate enough in the difficult and complex urban sound environments to be feasible for this purpose. 

This second point relates to one of the key differences between the DYNAMAP dataset and the \gls{isd} data; although the recordings provided by the DYNAMAP team were real recordings of a complex sound environment, they were manually selected as having a single dominant sound source and were much shorter (only 1--2.7s). By contrast, the \gls{isd} data are more representative of the total sound environment, meant to reflect all that the respondent was exposed to while completing the questionnaire. This means it contains many overlapping sound sources over 30s to 1 minute, and assumes that the respondents were responding to all of this. There may have been a single dominant sound source driving their response, or the soundscape may have been very heterogeneous. Effectively, this represents a weakly-labelled machine learning recognition task for the models, but how these labels are combined to give a usable label for the soundscape prediction model is less clear. Note that this is distinct from the recognition challenges for polyphonic sound event recognition task itself. What we are trying to solve is how to summarise a 30s recording containing many sound sources into a single label which best encapsulates the effect of the sound sources. This is the challenge facing a true soundscape prediction model based on the \gls{isd} which aims to reflect how people are exposed to a soundscape in a public space. 

This challenge could be tackled in a variety of ways, which no doubt will be developed as the field progresses, but I will propose some possible solutions. Two options would directly use the output from the recognition models. The simplest is to identify the sound with the highest output weighting over the entire recording period and consider this the `dominant' sound which then feeds into the soundscape model. Similarly, using the results from the recognition model, if the dominant sound in each time step is identified, then we could use the sound label which was identified for the plurality of the time steps, meaning it was the dominant sound for the most time. A similar, but perhaps more sophisticated approach takes inspiration from sleep disturbance. It is now common in sleep disturbance studies, particularly due to aircraft noise, to include information on the number of noise events, usually defined as the number of events with an $L_{A,max}$ which exceeds some threshold (e.g. $>60$dB) \citep{Janssen2014effect}. We could thus define a threshold level and define sound events as a contiguous period which exceeds the threshold; the recognition model would then be run on the longest contiguous period, giving some indication of the single sound which was noticeable for the longest time throughout the recording. 

A more sophisticated approach is to define so-called \emph{sound source profiles} \citep{Kang2018model,Berglund2006Tool}. This would be a set of categorical labels which characterise some particular combination of sound sources; for example, the `Traffic noise A profile' may be dominated by road traffic sounds with some amount of human voices and sirens present, while the `Traffic noise B profile' might be dominated by road traffic with loading and unloading sounds and no sirens. Similar specific subprofiles would also be defined for human and natural sound dominated profiles. Since these are categorical, they can easily be incorporated into the multi-level model given in \cref{eqn:integSource}. Although a basic version of these profiles could be defined using the sound source dominance questions in the \gls{ssid} protocol questionnaire (see \cref{app:questionnaire}), defining and subsequently assigning recordings to these profiles using the results of the environmental sound recognition models would seem preferable. By calculating e.g. the top ten identified sound sources, an unsupervised clustering algorithm with an appropriate method of determining the optimal number of clusters could help us to define these profiles based on patterns in the sound source data. Once the clusters are defined based on the output of the recognition models, it would be trivial to then process new recordings and classify them into their correct profile, then subsequently feeding this into the soundscape model. In this way, we can define a fairly comprehensive set of possible profiles of sound sources that are often seen together in urban environments and train a predictive soundscape model which can alter its psychoacoustic coefficients depending on the combination of sound sources present. At all stages throughout this process (except the internal calculation of the source recognition model), the definitions used, and the coefficients used throughout the model are transparent, understandable, and traceable. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}

In this chapter, an online listening experiment was conducted with 100 participants to assess the noise annoyance induced by short recordings of individual environmental noise sources gathered via a wireless acoustic sensors network in Milan. The main conclusions of this study are:

\begin{itemize}
  \item The acoustic samples gathered from selected sensors in Milan \gls{wasn} of the DYNAMAP project led the DYNAMAP team to a structured \gls{mushra} test to evaluate the annoyance in an offline perceptual test.
  \item When considering short recordings of single-source environmental sounds, no significant differences in noise annoyance were observed as a function of demographic factors, such as gender and self-reported area of residence (i.e. from very quiet to very noisy).
  \item The multi-level linear regression model derived from this case study achieved an overall $R^2=0.64$, using sharpness as a fixed effect (the first level), and impulsiveness, roughness, and tonality as random effects allowed to vary according to the type of sound (the second level) as predictors for perceived noise annoyance.
\end{itemize}

By using a consistent \gls{mlm} modelling strategy, the approach taken in this study highlights how a similar approach can be integrated into the general model. By incorporating sound source information along with the psychoacoustic metrics, the model can better reflect how listeners will respond to different sounds. The results given here demonstrate that the psychoacoustic features of a sound are most important in terms of how they cause us to perceive a certain sound, not on their own, separate from the semantic meaning we assign to a source. By allowing the relationship between psychoacoustic features and annoyance to vary per sound source, we create a more representative analogue of the perceptual mapping from soundscape indicators to descriptors. 

Given the somewhat unexpected result from this study that demographic factors had little difference on annoyance ratings, the next chapter will further investigate the influence of personal factors on soundscape perception making use of the larger and more diverse dataset from the \gls{isd}.
